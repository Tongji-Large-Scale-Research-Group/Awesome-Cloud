# Awesome-Cloud 周刊（第 7 期）：RDMA over Ethernet for Distributed AI Training at Meta Scale


这里简单记录每周分享的前沿内容，不定期发布。

注意本内容整理于2025年1月，其中存在许多个人的主观看法，仅供参考。

### 笔记整理：RDMA over Ethernet for Distributed AI Training at Meta Scale

---

#### **1. 背景与需求**
- **分布式大模型训练**  
  - 模型和数据分片到多个GPU，通过并行计算加速训练。  
  - 每次迭代需同步梯度或参数（梯度同步），需在毫秒级完成大量数据传输。  
  - 示例：Llama3在24,000 GPU集群中使用了16,000 GPU进行训练。  

- **通信技术**  
  - **RDMA（远程直接内存访问）**：绕过CPU，直接内存传输，效率高于传统TCP/IP。  
  - **Verbs API**：实现RDMA操作（如write/read）。  
  - **集体通信库（Collective Communication）**：分布式训练与网卡间的软件抽象层，通过Verbs API调度操作。  

- **工作负载特征**  
  - **作业规模**：当前8-256 GPU为主，未来将扩展至更大规模（如数万GPU）。  
  - **Collective操作规模**：单次操作通常涉及16-128 GPU，消息大小因模型差异显著。  

---

#### **2. 挑战**
- **网络架构限制**  
  - **TCP/IP**：CPU开销高，性能瓶颈明显。  
  - **IB（InfiniBand）**：需全栈更改设备与算法，成本高且不灵活。  

- **流量特征**  
  - **低熵**：流量分布不均，难以拆分调度。  
    - QP（队列对）数量反映流量多样性：QP少时流量集中，QP多时流量分散。  
  - **突发性（大象流）**：毫秒级“开-关”模式，瞬间可达网卡线路速率。  

- **Collective操作问题**  
  - **非最优逻辑拓扑**：如AllReduce的环状/树状结构可能引发拥塞。  
  - **协议假设偏差**：NCCL等库假设低延迟，实际生产环境可能不成立。  

- **RoCE拥塞管理不足**  
  - **DCQCN（数据中心量化拥塞控制）**的问题：  
    - 调优困难：严格ECN阈值降低吞吐量，宽松阈值恶化拥塞指标。  
    - 400G网络固件错误：CNP计数不准确。  

---

#### **3. 创新点**
针对以下研究挑战提出解决方案：  
1. **架构构建难**：优化传统网络架构限制。  
2. **流量均衡难**：解决分布式信息交换的低熵与突发性流量问题。  
3. **拥塞管理难**：改进Collective操作逻辑与RoCE拥塞算法（如DCQCN）。  

---

#### **4. 总结**
- **目标**：通过RDMA over Ethernet提升分布式AI训练的通信效率。  
- **核心问题**：网络架构、流量调度、拥塞管理。  
- **未来方向**：适应更大规模GPU集群，优化Collective操作与拥塞控制算法。  

**汇报信息**  
- 作者：王家晟  
- 导师：丁志军  
- 会议：ACM SIGCOMM (CCF-A)  
- 时间：2025年1月16日

## 问答环节记录

Q. 我看刚刚他那个效果。那不同模型它里面的那个就是 ALL TO ALL 之类的操作的含量其实是不太一样的，他是不是会比较就是换了一个模型的 DDP 效果就不会这么好了。
？
A. 对。


---

Q. 想问一下那个 collect 的那几种方法，就是进一步希望进一步介绍一下这几种方法。
A. 这个可能得去查 NCC 库了。


---

Q. 最近说这个纯算力调度的真的很少，但是大家对算网协同的考虑会挺多的，那但我之前看的是存算协同的虑比较多，那我想问问，从你们业绩的角度来说，比如你们最近干的事情来说是纯算力的多呢？还是说算网协同的会比较多，或者说结合的会比较多？

A. 可能会去做一些那种算子融合之类的这种优化，但是就整个那个 GPU 到底能算多快，还是靠英伟达来优化？

